[project]
name = "dlkit"
version = "0.1.0-alpha"
description = "custom modules and networks for pytorch integration with optuna and mlflow"
readme = "README.md"

authors = [
    { name = "constatza", email = "29166969+constatza@users.noreply.github.com" },
]

requires-python = ">=3.12, <3.14"
dependencies = [
    "numpy<3.0,>=2.1",
    "pathlib<2.0.0,>=1.0.1",
    "tomlkit<1.0.0,>=0.12.5",
    "pydantic<3.0.0,>=2.7.4",
    "lightning<3.0.0,>=2.3.2",
    "mlflow>=3.0,<4.0",
    "mlflow-skinny>=3.0,<4.0",
    "optuna<4,>=3",
    "torch>=2.4.0, <2.7.0",
    "dynaconf>=3.2.10",
    "pytest-mock>=3.14.0",
    "click>=8.1.8",
    "loguru>=0.7.3",
    "polars>=1.4.1,<2.0.0",
    "tables>=3.10.2",
    "pytorch-forecasting>=1.3.0",
    "psutil>=7.0.0",
    "torchmetrics>=1.7.1",
    "pydantic-settings>=2.10.1",
]

[dependency-groups]
dev = [
    "pre-commit>=4.2.0",
    "pytest<9.0.0,>=8.2.2",
]


[project.optional-dependencies]
plot = ["matplotlib<4.0.0,>=3.9.1", ]

[project.scripts]
server = "dlkit.scripts.start_mlflow_server:main"
train = "dlkit.scripts.training:main"

[tool.uv.sources]
torch = [
    { index = "pytorch-cpu", marker = "sys_platform == 'darwin'" },
    { index = "pytorch-gpu", marker = "sys_platform != 'darwin'" },
]


[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-gpu"
url = "https://download.pytorch.org/whl/cu124"
explicit = true


[build-system]
requires = ["hatchling>=1.0.0"]
build-backend = "hatchling.build"


[tool.pytest.ini_options]
addopts = [
    "--import-mode=importlib",
]
pythonpath = [
    "src",
]


[tool.ruff]
line-length = 100
target-version = "py313"
preview = true
fix = true

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
docstring-code-format = true

[tool.ruff.lint]
extend-select = [
    "UP", # pyupgrade
    #    "D", # pydocstyle
]



[tool.ruff.lint.pydocstyle]
convention = "google"
